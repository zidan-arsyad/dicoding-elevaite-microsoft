{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition\n",
    "\n",
    "Named Entity Recognition (NER) extracts entities like names, locations, organizations, and dates from text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods\n",
    "\n",
    "Method|Speed|Accuracy|Best Use Case|Confidence Score\n",
    "---|:---:|:---:|---|:---:\n",
    "NLTK|🐢 Slow|🔹 Decent|Rule-based NER, offline use|❌ Not available\n",
    "spaCy|⚡ Fast|🔹 Good|General NER tasks|🔹 Approximate\n",
    "Transformers (BERT, RoBERTa)|🐌 Slower|🔥 Best|High-accuracy, domain-specific tasks|✅ Yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. NLTK (Rule-based approach)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pros:\n",
    "1. Works offline\n",
    "2. Good for rule-based NER\n",
    "\n",
    "Cons:\n",
    "1. Slower than spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elon -> (PERSON)\n",
      "Musk -> (ORGANIZATION)\n",
      "CEO of Tesla -> (ORGANIZATION)\n",
      "SpaceX -> (ORGANIZATION)\n",
      "Pretoria -> (GPE)\n",
      "South Africa -> (GPE)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "\n",
    "downloaded = True\n",
    "# Download necessary resources\n",
    "if not downloaded:\n",
    "    nltk.download('maxent_ne_chunker_tab')\n",
    "    nltk.download('words')\n",
    "    nltk.download('punkt')\n",
    "\n",
    "\n",
    "# Sample text\n",
    "text = \"Elon Musk is the CEO of Tesla and SpaceX. He was born in Pretoria, South Africa, on June 28, 1971.\"\n",
    "\n",
    "# Tokenize and extract entities\n",
    "tokens = word_tokenize(text)\n",
    "pos_tags = pos_tag(tokens)\n",
    "tree = ne_chunk(pos_tags)\n",
    "\n",
    "# Extract named entities\n",
    "for subtree in tree:\n",
    "    if hasattr(subtree, \"label\"):\n",
    "        entity_name = \" \".join([token for token, pos in subtree.leaves()])\n",
    "        entity_type = subtree.label()\n",
    "        print(f\"{entity_name} -> ({entity_type})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. SpaCy with workaround\n",
    "\n",
    "SpaCy does not provide built-in confidence scores, but we can estimate them using NER token probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pros:\n",
    "1. Fast and efficient\n",
    "2. Pre-trained models available\n",
    "3. Supports visualization\n",
    "\n",
    "Cons:\n",
    "1. No built-in confidence score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elon Musk -> PERSON (Confidence: -20.00)\n",
      "Tesla -> ORG (Confidence: -20.00)\n",
      "Pretoria -> GPE (Confidence: -20.00)\n",
      "South Africa -> GPE (Confidence: -20.00)\n",
      "June 28, 1971 -> DATE (Confidence: -20.00)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Elon Musk\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " is the CEO of \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Tesla\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " and SpaceX. He was born in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Pretoria\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    South Africa\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ", on \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    June 28, 1971\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample text\n",
    "text = \"Elon Musk is the CEO of Tesla and SpaceX. He was born in Pretoria, South Africa, on June 28, 1971.\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Extract named entities with confidence scores\n",
    "for ent in doc.ents:\n",
    "    confidence = max(token.prob for token in ent) # Approx confidence score\n",
    "    print(f\"{ent.text} -> {ent.label_} (Confidence: {confidence:.2f})\")\n",
    "\n",
    "# Render named entities\n",
    "displacy.render(doc, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "💡 Note: spaCy’s confidence is approximate since token.prob represents a token's likelihood, not an explicit NER confidence score.\n",
    "\\\n",
    "- Probabilities are in log-space (negative values are normal).\n",
    "- Named entities tend to have low probability (e.g., -20) since they are less frequent in training data.\n",
    "- Common words have values closer to 0 (e.g., \"the\" ≈ -3).\n",
    "- Extremely rare words or out-of-vocabulary words may have values below -30."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hugging-face transformer (BERT-based NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El -> I-PER (Confidence: 1.00)\n",
      "##on -> I-PER (Confidence: 1.00)\n",
      "Mu -> I-PER (Confidence: 1.00)\n",
      "##sk -> I-PER (Confidence: 1.00)\n",
      "Te -> I-ORG (Confidence: 1.00)\n",
      "##sla -> I-ORG (Confidence: 1.00)\n",
      "Space -> I-ORG (Confidence: 1.00)\n",
      "##X -> I-ORG (Confidence: 1.00)\n",
      "Pre -> I-LOC (Confidence: 1.00)\n",
      "##toria -> I-LOC (Confidence: 1.00)\n",
      "South -> I-LOC (Confidence: 1.00)\n",
      "Africa -> I-LOC (Confidence: 1.00)\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load pre-trained NER model\n",
    "ner = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "\n",
    "# Sample text\n",
    "text = \"Elon Musk is the CEO of Tesla and SpaceX. He was born in Pretoria, South Africa, on June 28, 1971.\"\n",
    "\n",
    "# Extract named entities\n",
    "entities = ner(text)\n",
    "\n",
    "# Print result with confidence scores\n",
    "for entity in entities:\n",
    "    entity_name = entity[\"word\"]\n",
    "    entity_type = entity[\"entity\"]\n",
    "    confidence = entity[\"score\"]\n",
    "    print(f\"{entity_name} -> {entity_type} (Confidence: {confidence:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output warning\n",
    "\n",
    "> Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
    "> - This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
    "> - This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
    "\n",
    "\\\n",
    "\\\n",
    "**Why Are Some Weights Unused?**\n",
    "\n",
    "BERT is a multi-purpose model, and it has different layers for different NLP tasks:\n",
    "Component|Purpose|Used in NER?\n",
    "---|---|---\n",
    "Embedding Layer|Converts words into vector representations|✅ Yes\n",
    "Transformer Layers|Contextualizes words|✅ Yes\n",
    "Token Classification Layer|Classifies each word/token into an entity|✅ Yes\n",
    "Pooler Layer (bert.pooler.dense weights)|Used for tasks like text classification (summarizes sentence into one vector)|❌ No\n",
    "\n",
    "In NER, we classify each token separately, so we don’t need the pooler.dense weights, which are used to represent entire sentences in classification tasks (e.g., sentiment analysis).\n",
    "\n",
    "This warning is expected because `bert.pooler` is for text classification, not token classification.\n",
    "\n",
    "\\\n",
    "\\\n",
    "**How to suppress the warning?**\n",
    "\n",
    "`import transformers`\\\n",
    "`transformers.logging.set_verbosity_error()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Fix: Merge subwords tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "📌 Why Are Words Split (e.g., \"El\", \"##on\") in Hugging Face NER Results?\n",
    "\n",
    "This happens because Hugging Face’s Transformer models use WordPiece tokenization, which breaks words into subword units. Tokens prefixed with ## are continuations of previous subwords.\n",
    "\\\n",
    "Hugging Face models like BERT and RoBERTa use subword tokenization to handle rare or unknown words efficiently.\n",
    "\n",
    "For example:\n",
    "- \"Elon\" → Split into [\"El\", \"##on\"]\n",
    "- \"Musk\" → Split into [\"Mu\", \"##sk\"]\n",
    "- \"Tesla\" → Split into [\"Te\", \"##sla\"]\n",
    "- \"SpaceX\" → Split into [\"Space\", \"##X\"]\n",
    "\n",
    "This ensures better handling of uncommon words, but it makes the output look fragmented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_subwords(entities):\n",
    "    merged_entities = []\n",
    "    current_word = \"\"\n",
    "    current_label = None\n",
    "    current_score = 0\n",
    "\n",
    "    for entity in entities:\n",
    "        word = entity[\"word\"]\n",
    "        label = entity[\"entity\"]\n",
    "        score = entity[\"score\"]\n",
    "\n",
    "        # If the word starts with \"##\", it's a continuation of the previous token\n",
    "        if word.startswith(\"##\"):\n",
    "            current_word += word[2:] # Remove \"##\" and append\n",
    "            current_score = max(current_score, score) # Keep the highest score\n",
    "        else:\n",
    "             # Save previous word before starting a new one\n",
    "            if current_word:\n",
    "                merged_entities.append((current_word, current_label, current_score))\n",
    "            \n",
    "            current_word = word\n",
    "            current_label = label\n",
    "            current_score = score\n",
    "            \n",
    "    if current_word:\n",
    "        merged_entities.append((current_word, current_label, current_score))\n",
    "        \n",
    "    return merged_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elon -> B-PER (Confidence: 0.98)\n",
      "Musk -> I-PER (Confidence: 1.00)\n",
      "Tesla -> B-ORG (Confidence: 1.00)\n",
      "and -> I-ORG (Confidence: 0.61)\n",
      "SpaceX -> B-ORG (Confidence: 1.00)\n",
      "Pretoria -> B-LOC (Confidence: 1.00)\n",
      "South -> B-LOC (Confidence: 1.00)\n",
      "Africa -> I-LOC (Confidence: 1.00)\n"
     ]
    }
   ],
   "source": [
    "merged_entities = merge_subwords(entities)\n",
    "\n",
    "# Print result with confidence scores\n",
    "for entity in merged_entities:\n",
    "    entity_name = entity[0]\n",
    "    entity_type = entity[1]\n",
    "    confidence = entity[2]\n",
    "    print(f\"{entity_name} -> {entity_type} (Confidence: {confidence:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Fix: Date recognition *(not really)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Hugging Face model (dbmdz/bert-large-cased-finetuned-conll03-english) that was used\\\n",
    "was trained on the CoNLL-2003 dataset, which only recognizes four entity types:\n",
    "\n",
    "Entity Type|Meaning\n",
    "---|---\n",
    "PER||Person (e.g., \"Elon Musk\")\n",
    "ORG||Organization (e.g., \"Tesla\")\n",
    "LOC||Location (e.g., \"South Africa\")\n",
    "MISC||Miscellaneous (e.g., \"SpaceX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elon -> B-PER (Confidence: 0.98)\n",
      "Musk -> I-PER (Confidence: 1.00)\n",
      "Tesla -> B-ORG (Confidence: 1.00)\n",
      "SpaceX -> B-ORG (Confidence: 1.00)\n",
      "Pretoria -> B-LOC (Confidence: 1.00)\n",
      "South -> B-LOC (Confidence: 1.00)\n",
      "Africa -> I-LOC (Confidence: 1.00)\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import transformers\n",
    "transformers.logging.set_verbosity_error() # Disable warnings\n",
    "\n",
    "# Load pre-trained NER model\n",
    "ner_pipeline = pipeline(\"ner\", model=\"dslim/bert-base-NER\")\n",
    "\n",
    "# Sample text\n",
    "text = \"Elon Musk is the CEO of Tesla and SpaceX. He was born in Pretoria, South Africa, on June 28, 1971.\"\n",
    "\n",
    "# Extract named entities\n",
    "entities = ner_pipeline(text)\n",
    "\n",
    "# Merge subwords\n",
    "merged_entities = merge_subwords(entities)\n",
    "\n",
    "# Print result with confidence scores\n",
    "for entity in merged_entities:\n",
    "    entity_name = entity[0]\n",
    "    entity_type = entity[1]\n",
    "    confidence = entity[2]\n",
    "    if confidence > 0.85:\n",
    "        print(f\"{entity_name} -> {entity_type} (Confidence: {confidence:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note\n",
    "The date is still missing, consider to use Flair model (\"flair/ner-english-large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Legends\n",
    "\n",
    "Named Entity Recognition (NER) models use the IOB tagging scheme to identify entities in text.\n",
    "Tag|Meaning\n",
    ":--:|---\n",
    "B-XXX|Begin: Start of an entity (e.g., first word of a location name)\n",
    "I-XXX|Inside: Continuation of an entity (e.g., second word in a multi-word location)\n",
    "O|Outside: Not part of any entity\n",
    "\n",
    "💡 The B- tag helps the model differentiate between separate entities.\n",
    "\\\n",
    "\\\n",
    "**Example**:\\\n",
    "\"I visited New York and Los Angeles\"\n",
    "Word|Correct Tag|Incorrect Tag (Without B-LOC)\n",
    "---|:---:|---\n",
    "New|B-LOC|I-LOC\n",
    "York|I-LOC|I-LOC\n",
    "and|O|I-LOC ❌ (Wrongly included in New York)\n",
    "Los|B-LOC|I-LOC ❌ (Still part of New York)\n",
    "Angeles|I-LOC|I-LOC\n",
    "\n",
    "✅ Using B-LOC ensures that \"Los Angeles\" is recognized as a separate entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
