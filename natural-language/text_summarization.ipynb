{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types\n",
    "There are two main types of summarization:\n",
    "1. Extractive Summarization – Selects key sentences directly from the text.\n",
    "2. Abstractive Summarization – Generates new sentences while keeping the original meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods\n",
    "\n",
    "Method|Best For|Pros|Cons\n",
    "---|---|---|---\n",
    "Extractive (spaCy)|Quick sentence selection|Fast, lightweight, no training needed|Less flexible, no paraphrasing\n",
    "Abstractive (Hugging Face BART/T5)|Human-like summaries|More natural, deep learning-based|Slower, requires a large model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Extractive summarization with spaCy\n",
    "\n",
    "Extracts important sentences without modifying the wording."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pros:** Fast, doesn’t require training.\\\n",
    "**Cons:** Just selects sentences, no rewording."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from heapq import nlargest\n",
    "\n",
    "\n",
    "def extractive_summary(text, num_sentences=5):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Calculate word frequencies\n",
    "    word_frequencies = {}\n",
    "    for word in doc:\n",
    "        if word.text.lower() not in STOP_WORDS and word.text.isalpha():\n",
    "            word_frequencies[word.text] = word_frequencies.get(word.text, 0) + 1\n",
    "\n",
    "    max_frequencies = max(word_frequencies.values())\n",
    "\n",
    "    # Normalize word frequencies\n",
    "    for word in word_frequencies:\n",
    "        word_frequencies[word] /= max_frequencies\n",
    "\n",
    "    # Score sentences based on word frequencies\n",
    "    sentence_scores = {}\n",
    "    for sent in doc.sents:\n",
    "        for word in sent:\n",
    "            if word.text in word_frequencies:\n",
    "                sentence_scores[sent] = (\n",
    "                    sentence_scores.get(sent, 0) + word_frequencies[word.text]\n",
    "                )\n",
    "\n",
    "    # Get the top sentences\n",
    "    summarized_sentences = nlargest(\n",
    "        num_sentences, sentence_scores, key=sentence_scores.get\n",
    "    )\n",
    "    summary = \" \".join([sent.text for sent in summarized_sentences])\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Researchers continue to develop responsible AI frameworks to ensure fairness, transparency, and accountability in AI-driven decision-making. Early AI systems focused on rule-based approaches and expert systems, which, although powerful for specific tasks, lacked the adaptability of modern machine learning models. The journey of AI began in the mid-20th century when pioneers like Alan Turing and John McCarthy laid the theoretical foundations for machine intelligence.\n"
     ]
    }
   ],
   "source": [
    "sample_long_text = \"\"\"\n",
    "    Artificial Intelligence (AI) has rapidly evolved over the past few decades, transforming industries, reshaping economies, and revolutionizing human interactions with technology. The journey of AI began in the mid-20th century when pioneers like Alan Turing and John McCarthy laid the theoretical foundations for machine intelligence. Early AI systems focused on rule-based approaches and expert systems, which, although powerful for specific tasks, lacked the adaptability of modern machine learning models.\n",
    "    With the rise of deep learning in the 2010s, AI took a significant leap forward. Neural networks, inspired by the structure of the human brain, enabled machines to recognize speech, translate languages, and even generate realistic images. Companies like Google, OpenAI, and Tesla leveraged deep learning to create state-of-the-art AI applications. Self-driving cars, natural language processing (NLP), and recommendation algorithms became mainstream.\n",
    "    Despite these advancements, AI still faces ethical and technical challenges. Bias in AI models, data privacy concerns, and the impact of automation on employment are widely debated topics. Researchers continue to develop responsible AI frameworks to ensure fairness, transparency, and accountability in AI-driven decision-making.\n",
    "    Looking ahead, AI is expected to become even more integrated into daily life. Innovations in healthcare, education, and robotics promise to enhance human capabilities while raising important ethical considerations. As AI progresses, balancing innovation with ethical responsibility will be crucial for shaping a future where artificial intelligence benefits all of humanity.\n",
    "\"\"\"\n",
    "sample_long_text = sample_long_text.replace(\"\\n\", \"\")\n",
    "sample_long_text = sample_long_text.replace(\"    \", \"\")\n",
    "\n",
    "# Generate a summary of the sample text\n",
    "summary = extractive_summary(sample_long_text, num_sentences=3)\n",
    "\n",
    "# Print the summary\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Abstractive summarization with Hugging Face Transformer\n",
    "\n",
    "Uses pre-trained deep learning models (BART/T5) to generate a human-like summary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pros:** More natural and human-like.\\\n",
    "**Cons:** Requires a pre-trained model (larger and slower)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The journey of AI began in the mid-20th century when pioneers like Alan Turing and John McCarthy laid the theoretical foundations for machine intelligence . Early AI systems focused on rule-based approaches and expert systems . With the rise of deep learning in the 2010s, AI took a significant leap forward .\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the summarization pipeline\n",
    "summarizer = pipeline(\"summarization\")\n",
    "\n",
    "# Generate a summary using the pipeline\n",
    "summary = summarizer(sample_long_text, max_length=100, min_length=20, do_sample=False)\n",
    "\n",
    "# Print the summary\n",
    "print(summary[0][\"summary_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
