{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification\n",
    "\n",
    "Use cases:\n",
    "1. Spam detection\n",
    "2. Topic labeling\n",
    "3. Sentiment analysis\n",
    "4. etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to use each models\n",
    "\n",
    "|Method|Best For|Complexity|Data Requirement|\n",
    "|---|---|:---:|:---:|\n",
    "|Rule-Based|Simple keyword-based tasks|Low|None|\n",
    "|Naïve Bayes|Spam detection, sentiment analysis|Medium|Small dataset|\n",
    "|SVM/Logistic Regression|Topic classification|Medium|Medium-sized dataset|\n",
    "|Deep Learning (BERT, LSTMs)|Large-scale, complex text tasks|High|Large labeled dataset|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Rule-based method\n",
    "\n",
    "Pros:\n",
    "1. Simple\n",
    "2. Interpretable\n",
    "\n",
    "Cons:\n",
    "1. Need rule updates\n",
    "2. Can't generalize well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified email as: ['spam', 'spam', 'spam', 'not spam', 'spam', 'not spam']\n"
     ]
    }
   ],
   "source": [
    "def classify_email(email):\n",
    "    email = email.lower()\n",
    "    spam_keywords = [\"free money\", \"lottery\", \"prize\", \"win\", \"jackpot\"]\n",
    "    for keyword in spam_keywords:\n",
    "        if keyword in email:\n",
    "            return \"spam\"\n",
    "    return \"not spam\"\n",
    "\n",
    "\n",
    "sample_emails = [\n",
    "    \"Get your free money!\",\n",
    "    \"Obtain huge jackpot!\",\n",
    "    \"Guaranteed to win 50x\",\n",
    "    \"Please resubmit your form\",\n",
    "    \"You have won a prize\",\n",
    "    \"You have been selected\",\n",
    "]\n",
    "\n",
    "print(\"Classified email as:\", list(map(classify_email, sample_emails)))\n",
    "# Output: ['spam', 'spam', 'spam', 'not spam', 'spam', 'not spam']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ML-based method\n",
    "Common algorithms:\n",
    "1. Naïve Bayes: Good for spam filtering and topic classification.\n",
    "2. Support Vector Machines (SVM): Works well with small datasets.\n",
    "3. Logistic Regression: Simple and effective for binary classification.\n",
    "4. Random Forest: Uses decision trees to improve accuracy.\n",
    "5. Deep Learning (LSTMs, CNNs, Transformers): Best for complex text data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm differences\n",
    "\n",
    "Model|How It Works|Pros|Cons|Best Use Cases\n",
    "---|---|---|---|---\n",
    "Naïve Bayes (NB)|Uses Bayes' theorem to compute probabilities of a text belonging to a class|Fast, works well with small datasets, handles noisy data|Assumes word independence, doesn’t capture relationships between words|Spam filtering, sentiment analysis, simple text classification\n",
    "Support Vector Machines (SVM)|Finds a hyperplane that best separates classes in high-dimensional space|Works well with small datasets, robust to overfitting|Computationally expensive for large data, doesn’t capture word order|Topic classification, sentiment analysis, document categorization\n",
    "Logistic Regression|Uses a linear function to model class probabilities|Simple and interpretable, efficient on small datasets|Limited to linearly separable data, doesn’t handle long-range dependencies|Binary classification (spam vs. not spam), fake news detection\n",
    "Random Forest|Uses multiple decision trees to make predictions|Works with imbalanced data, robust to noise|Can be slow on large datasets, requires feature engineering|Multi-class classification, author attribution, keyword-based classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Sentiment analysis using Naïve Bayes\n",
    "Pros:\n",
    "1. Can handle large dataset\n",
    "2. More accurate than rule-based method\n",
    "\n",
    "Cons:\n",
    "1. Requires labeled data\n",
    "2. Needs training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted sentiment: ['negative' 'negative' 'positive']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# sample dataset\n",
    "texts = [\n",
    "    \"I love this product\",\n",
    "    \"This is awesome\",\n",
    "    \"I feel great\",\n",
    "    \"I am so happy\",\n",
    "    \"I am not happy\",\n",
    "    \"I feel terrible\",\n",
    "    \"I hate this product\",\n",
    "    \"I don't like it\",\n",
    "    \"I'm terribly in love with this product\",\n",
    "    \"I loved the fact that I wasted money on this product\",\n",
    "    \"This product broke after 1 day\",\n",
    "    \"I love that this product last for 2 years\",\n",
    "    \"This product last for eternity\",\n",
    "]\n",
    "labels = [\n",
    "    \"positive\",\n",
    "    \"positive\",\n",
    "    \"positive\",\n",
    "    \"positive\",\n",
    "    \"negative\",\n",
    "    \"negative\",\n",
    "    \"negative\",\n",
    "    \"negative\",\n",
    "    \"positive\",\n",
    "    \"negative\",\n",
    "    \"negative\",\n",
    "    \"positive\",\n",
    "    \"positive\",\n",
    "]\n",
    "\n",
    "# create a pipeline\n",
    "model = make_pipeline(CountVectorizer(), MultinomialNB())\n",
    "model.fit(texts, labels)\n",
    "\n",
    "# predict the sentiment of a new text\n",
    "new_text = [\n",
    "    \"I feel happy that I wasted 10 hours in line\",\n",
    "    \"I hate that I wasted my time and money\",\n",
    "    \"I love that this product broke after 2 days\",\n",
    "]\n",
    "\n",
    "print(\"Predicted sentiment:\", model.predict(new_text))\n",
    "# Expected output: ['negative', 'negative', 'negative']\n",
    "# Actual output: ['negative', 'negative', 'positive']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Deep learning-based method\n",
    "Uses neural network for text classification<br>\n",
    "\n",
    "Popular models:\n",
    "1. RNN (LSTMs, GRUs): Handles sequential text well.\n",
    "2. CNN for Text: Extracts features from text for classification.\n",
    "3. Transformers (BERT, GPT, RoBERTa): State-of-the-art models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm differences\n",
    "\n",
    "Model|How It Works|Pros|Cons|Best Use Cases\n",
    "---|---|---|---|---\n",
    "Recurrent Neural Networks (RNNs)|Processes text sequentially, maintaining a memory of previous words|Captures context better than ML models, good for sequence-based tasks|Struggles with long sentences due to vanishing gradients|Sequential text classification (e.g., chatbot intent recognition)\n",
    "Long Short-Term Memory (LSTM)|A type of RNN that retains long-term dependencies using gates|Handles longer sequences, avoids vanishing gradient problem|Computationally expensive, slow training|Sentiment analysis, document classification, named entity recognition\n",
    "Gated Recurrent Units (GRU)|A simplified LSTM with fewer parameters|Faster training than LSTM, performs well on medium-length text|Still slower than CNNs and Transformers|Similar use cases as LSTM but with better efficiency\n",
    "Convolutional Neural Networks (CNNs) for Text|Uses filters to detect patterns in word embeddings (n-grams)|Faster training than RNNs, works well with short text|Doesn’t capture word order dependencies well|Short text classification (e.g., fake news detection, sentiment analysis)\n",
    "Transformers (BERT, GPT, RoBERTa, etc.)|Uses attention mechanisms to capture long-range dependencies in text|State-of-the-art performance, captures deep contextual meaning|Requires large datasets and GPUs, slow inference|Any advanced NLP task: document classification, chatbots, translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Text Classification with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted sentiment: ['POSITIVE', 'NEGATIVE', 'NEGATIVE', 'POSITIVE', 'NEGATIVE', 'NEGATIVE', 'POSITIVE', 'POSITIVE', 'POSITIVE', 'NEGATIVE', 'NEGATIVE']\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\n",
    "    \"text-classification\", model=\"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    ")\n",
    "\n",
    "sample_texts = [\n",
    "    \"I love this product\",\n",
    "    \"I hate this product\",\n",
    "    \"I don't like it\",\n",
    "    \"I'm terribly in love with this product\",\n",
    "    \"I loved the fact that I wasted money on this product\",\n",
    "    \"This product broke after 1 day\",\n",
    "    \"I love that this product last for 2 years\",\n",
    "    \"I feel happy that I wasted 10 hours in line\",\n",
    "    \"I feel happy that I spend 10 hours in line\",\n",
    "    \"I hate that I wasted my time and money\",\n",
    "    \"I love that this product broke after 2 days\",\n",
    "]\n",
    "\n",
    "prediction = classifier(sample_texts)\n",
    "\n",
    "print(\"Predicted sentiment:\", [p[\"label\"] for p in prediction])\n",
    "# Expected output: ['POSITIVE', 'NEGATIVE', 'NEGATIVE', 'POSITIVE', 'NEGATIVE', 'NEGATIVE', 'POSITIVE', 'NEGATIVE', 'POSITIVE', 'NEGATIVE', 'NEGATIVE']\n",
    "# Actual output: ['POSITIVE', 'NEGATIVE', 'NEGATIVE', 'POSITIVE', 'NEGATIVE', 'NEGATIVE', 'POSITIVE', 'POSITIVE', 'POSITIVE', 'NEGATIVE', 'NEGATIVE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML vs DL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key differences\n",
    "\n",
    "Feature|Traditional ML (NB, SVM, RF, etc.)|Deep Learning (LSTM, CNN, Transformers)\n",
    "---|---|---\n",
    "Feature Engineering|Requires manual features (TF-IDF, n-grams)|Learns features automatically\n",
    "Performance on Small Data|Works well|Needs large data for good performance\n",
    "Context Awareness|Doesn’t capture relationships between words well|Captures deep word relationships\n",
    "Training Time|Fast|Slow (especially Transformers)\n",
    "Computational Cost|Low|High (GPUs often needed)\n",
    "Generalization|Works well on simple tasks|Works well on complex tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to Use Which Model?\n",
    "    ✅ If you have a small dataset → Use Naïve Bayes, SVM, or Logistic Regression\n",
    "    ✅ If you need fast training → Use Random Forest or CNN\n",
    "    ✅ If you need context understanding → Use LSTM, GRU, or Transformers\n",
    "    ✅ If you have lots of data & computing power → Use BERT, GPT, or other Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
